\chapter{Implementación}

En este capítulo analizaremos la implementación realizada de los conceptos teóricos vistos hasta ahora y el uso práctico de las tecnologías involucradas.

\medskip

El diseño de la aplicación está basado en el patrón de software \emph{facade} con la idea de ofrecer una interfaz en C++ de tipo librería de alto nivel, reutilizable y fácil de usar, que permita realizar renderizados realistas sin tener que preocuparse de los detalles de bajo nivel relativos a OptiX y CUDA, ni de todos los conceptos teóricos involucrados.

\medskip

Además sobre esta interfaz se ha construido un parser de escenas XML, para hacer aún más sencillo el acto de configurar y crear escenas.

\clearpage

\section{Interfaz de programación}

\subsection{Estructura TCamera}

Este tipo de datos representa la cámara de la escena y encapsula todos los datos relativos a la cámara. Para construir un objeto de este tipo le pasamos la posición de la cámara, la posición del objetivo y el ángulo de campo de visión.

\subsection{TSphereLight}

Esta estructura representa una luz esférica, tomando como parámetros la posición del centro de la esfera, su radio y una tripleta RGB representando su emisión de luz.

\subsection{TObjModel}

Es una estructura que representa un modelo 3D en formato .obj. Se construye pasándole la ruta del modelo. 

\subsection{Clase CPathtracer}

Este es el tipo de datos más relevante de nuestra interfaz. Todos los datos relativos a la escena y al renderizado son finalmente encapsulados en un objeto de este tipo. Una vez configurado, es el responsable de cargar los programas de OptiX y construir el \emph{Context} de OptiX y controlar la ejecución del mismo.

\subsection{Interfaz de programación}

Para explicar el funcionamiento de la interfaz lo haremos mediante un sencillo ejemplo que contiene las clases y métodos principales de la interfaz.

\definecolor{orange}{rgb}{1.0,0.5,0.2}
\definecolor{purple}{rgb}{0.5,0.2,0.9}
\definecolor{blue}{rgb}{0.0,0.0,0.8}
\definecolor{green}{rgb}{0,0.7,0}
\definecolor{black}{rgb}{0.0,0.0,0.0}

\lstdefinestyle{customc}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  frame=L,
  language=C,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily,
  keywordstyle=\bfseries\color{green},
  commentstyle=\itshape\color{purple},
  identifierstyle=\color{blue},
  stringstyle=\color{orange},
}


\lstset{style=customc}

\begin{lstlisting}
CPathtracer pt;

pt.setRenderSize(width, height);
pt.setSqrtSamplesPerPixel(sqrtspp);
pt.setBlockSize(200u);
pt.setMaxDepth(7);


pt.setCamera(TCamera(optix::make_float3(8.0, 9.0, 1.0), 
		optix::make_float3(7.0, 9.0, 0.5), 60.0f));
pt.addObjModel(
		TObjModel("assets/dabrovic-sponza/sponza.obj"));
pt.addLight(TSphereLight(optix::make_float3(2.0f, 2.0f, 2.0f),
		optix::make_float3(0.0f, 5.0f, 0.0f), 1.0f));

pt.prepare();
pt.renderAllSamples();
pt.saveToTGA("render_sponza.tga");
\end{lstlisting}

Este ejemplo sirve para ilustrar el funcionamiento de la interfaz con el motor de renderizado.
Veamos el funcionamiento de este ejemplo:
En primer lugar creamos una instancia de la clase CPathtracer para seguidamente especificar las dimensiones, en pixeles, de la imagen que vamos a renderizar usando el método \emph{void setRenderSize(unsigned int width, unsigned int height)}. 

\medskip

Seguidamente usamos la instrucción \emph{pt.setSqrtSamplesPerPixel(sqrtspp} para especificar el número de muestras por pixel. El valor que toma este método es la raíz cuadrada del número de muestras. Lo hacemos de este modo para asegurarnos de que la división de cada pixel se hará en regiones uniformes, dividiendo el pixel en \emph{sqrtspp} filas y \emph{sqrtspp} columnas.

\medskip

En la siguiente línea, especificamos un tamaño aproximado de los bloques en los que dividiremos la imagen. Entraremos en más detalle sobre este comportamiento en las secciones siguientes, pero de momento quedémonos con la idea de que la instrucción \emph{pt.setBlockSize(200u);} le dice al renderizador que debe dividir la imagen en bloques de aproximadamente 200x200 pixels. 

\medskip

A continuación especificamos la profundidad de recursión máxima. Este valor equivale al número de veces que un rayo rebotará por la escena antes de ser terminado. Hay que calibrar bien este valor, ya que un valor muy bajo produciría una iluminación pobre de la escena y un valor demasiado alto incrementaría el tiempo de ejecución innecesariamente.

\medskip

Seguidamente le pasamos al pathtracer un objeto de tipo TCamara inicializado con los valores de posición, objetivo y ángulo de visión deseado. Sólo puede haber una cámara activa a la vez por lo que la clase CPathtracer siempre usará la última que le especifiquemos.

\medskip

Los métodos \emph{addObjModel} y \emph{addLight} añaden modelos .obj y luces a la escena. A diferencia de la cámara, podemos llamar a estos métodos tantas veces como queramos para añadir luces y geometría a la escena.

\medskip

El método \emph{prepare} toma todos los datos de configuración que hemos especificado anteriormente y los compila en un \emph{Context} de OptiX para poder empezar el proceso de renderizado.

\medskip

Usamos la instrucción \emph{pt.renderAllSamples()} para efectuar un renderizado completo, de la imagen que tomará tantas muestras como hayamos especificado, y por último guardamos el resultado en una fichero TGA. Por simplicidad en este ejemplo hemos usado el método \emph{renderAllSample}, pero también existe la opción de renderizar sólo una muestra por pixel por llamada con el método \emph{void renderNextSample()}. Este método será útil cuando queramos dar un feedback visual del progreso del renderizado, calculando una muestra y pintando en pantalla el resultado cada vez.

\clearpage

\section{Renderizado por bloques}

Durante el proceso de implementación nos encontramos con una dificultad cuando se trataba de renderizar escenas con muchos polígonos: el sistema operativo impone un límite al tiempo de ejecución de una llamada a la GPU de unos pocos segundos. Si llamábamos a la ejecución de un Context de OptiX contra toda la imagen, es decir con tantos CUDA threads como pixels, el sistema operativo paraba la ejecución antes de que esta terminara.

\medskip

La solución que encontramos fue dividir la imagen en bloques de menor tamaño y lanzar una ejecución del Context para cada uno de estos bloques. Es como pintar varias imágenes de menor tamaño y luego juntarlas, con la diferencia de que mantenemos un buffer en la memoria del device, del tamaño de la imagen final, para no tener que hacer transacciones de memoria entre el host y el device ya que esto reduciría notablemente el rendimiento.

\clearpage

\section{Algoritmo implementado}

En esta sección veremos el flujo de ejecución del algoritmo de pathtracing que hemos implementado. Tal como hemos visto en capitulos anteriores el algoritmo de pathtracing se basa en estimar la cantidad de luz captada por cada pixel de la imagen, tomando varias muestras y sumando la contribución de cada muestra.

\medskip

Con tal de reducir el numero de transacciones de memoria entre el host y el device creamos un buffer de salida en este ultimo donde se ira acumulando el valor de las muestras tomadas. Dicho buffer se mantiene en la memoria del device entre llamadas sucesivas al kernel.

\medskip

En la figura \ref{fig:algorithm} puede verse el diagrama de flujo para el trazado de cada path para obtener una estimación de la luz captada por el sensor en el pixel correspondiente.

\medskip

Tal como puede verse en el diagrama se trata de un algoritmo recursivo que consiste en hacer rebotar un rayo por la escena, obteniendo la dirección de rebote probabilísticamente según las propiedades del material. Este proceso se efectua hasta que el rayo es absorbido por un material, el rayo intersecciona con una fuente de luz o bien llegamos a la profundidad de recursión máxima especificada.

\begin{figure}
\label{fig:algorithm}
\centering
\includegraphics[height=8in]{algorithm.png}
\caption{Diagrama de flujo del algoritmo}
\vspace*{0.5in}
\end{figure}

\clearpage

\section{Cámara}

\subsection{Host}

En el host precalculamos los datos de la cámara que serán constantes durante la ejecución: los vectores de la base ortonormal del sistema de coordenadas de cámara (front, left, up) y la relación entre el vector front y el vector up en función del ángulo de campo de visión.

\subsection{Device}

En el device tenemos un programa encargado de generar los rayos primarios que se lanzaran desde la cámara, para ello se le pasan desde el host una serie de parámetros necesarios para lanzar el rayo correcto.

El programa toma un offset con las coordenadas absolutas en espacio de imagen del pixel top-left del bloque que estamos pintando y el índice del CUDA thread actual que se corresponde con las coordenadas relativas al bloque actual del pixel que estamos pintando. Usamos estas variables para calcular las coordenadas top-left del pixel que estamos pintando, en coordenadas absolutas de la imagen entera.

\medskip

Para que el muestreo múltiple de cada pixel proporcione antialiasing es necesario dividir el pixel en regiones y lanzar el rayo a través de la región que se corresponde con la muestra actual. Para calcular las coordenadas de la región actual lo hacemos a partir del indice de la muestra actual y de la raíz cuadrada del total de muestras.

\medskip

Efectuados estos cálculos tenemos las coordenadas top-left absolutas de la región de pixel. A estas coordenadas le sumamos el valor de media región de pixel para que el rayo salga por el centro de dicha región, en vez de por su esquina superior-izquierda.

\medskip

Normalizamos esas coordenadas para obtener las coordenadas de pantalla correctas en el intervalo $[-1, 1]$. Corregimos la relación de aspecto de la imagen y calculamos la dirección del rayo resultante.

\clearpage

\section{Muestreo de la BRDF}

Para el muestreo de la BRDF usamos el modelo propuesto en \cite{Lafortune1994} que hemos explicado en la sección \ref{muestreo_brdf}.
\medskip

En primer lugar decidimos aleatoriamente si muestrear la parte difusa o la parte especular de la BRDF. Para ello proponemos usar como probabilidades las medias de los valores RGB de las componentes difusa y especular: $p_{diff} =\frac{1}{3}( k_{dR} + k_{dG} + k_{dB})$ y $p_{spec} =\frac{1}{3}( k_{sR} + k_{sG} + k_{sB})$ 

\medskip

Generamos un número aleatorio $r$ en $[0,1]$ y si $r \leq p_{spec}$ muestreamos la parte especular, si $p_{spec} < r \leq p_{spec} + p_{diff}$ la parte difusa, en caso contrario el rayo es absorbido y terminamos la recursión del mismo.

\medskip

Esta forma de terminar la recursión se conoce como ruleta rusa y es una técnica de reducción de la varianza usado en integraciones por Montecarlo. Para compensar esta terminación anticipada y la separación entre rayos difusos y especulares, dividiremos el resultado entre la probabilidad de que se lance ese rayo. Ya que se muestrea con más probabilidad el evento más probable esto servirá para dar más peso a aquellas muestras menos probables.

\subsection{Muestreo de la parte difusa de la BRDF}
En el programa de closest hit, muestreamos la hemiesfera centrada en el punto de intersección para tener una estimación de la luz incidente.

\medskip

Básicamente lo que hacemos es, en el caso de que no hayamos llegado a la profundidad de recursión máxima, muestrear un punto en la hemiesfera con densidad de coseno, generar una base ortonormal en función de la normal de la superficie y transformar el punto de la hemiesfera con esa base. Se traza un rayo recursivamente en la dirección obtenida y el color devuelto por ese rayo se multiplica por el color difuso de la superficie.

\medskip

Hay que hacer notar que cuando calculamos la contribución de la luz incidente $L(x \to v) = L(x \gets l) * k_d / p_{diff}$ no estamos multiplicando por el típico factor coseno de la normal y la luz. No lo hacemos porque al tomar la muestra con densidad coseno y dividir entre la densidad de probabilidad de esa muestra los factores coseno se anulan.
Siguiendo con ese razonamiento y teniendo en cuenta que la densidad coseno es $\frac{cos(l,n)}{\pi}$ deberíamos multiplicar el resultado por $\pi$ para ser coherentes con la teoría, pero lo hemos quitado del cálculo porque hemos comprobado empíricamente que el resultado final es más satisfactorio sin ese factor, que en caso de incluirlo en el cálculo generaba imágenes excesivamente iluminadas y quemadas.

\medskip

Usando solamente esta función de iluminación ya es posible generar renderizados, aunque al no muestrear las luces directamente tardarán más en converger.
En las primeras fases de la implementación realizamos algunos renderizados con este closest hit program, y un closest hit program para las fuentes de luz que devolvía su emisión de luz. Un ejemplo de ello puede verse en la siguiente figura.


\begin{figure}[h]
\centering
\includegraphics[width=5in]{dome1.png}
\caption{Muestreo de la brdf difusa}
\end{figure}

\clearpage

\subsection{Muestreo de la parte especular de la BRDF}

En el caso de la parte especular usamos como densidad de probabilidad una potencia de coseno de forma que el exponente sea mayor cuanto más reflectante sea el material. Ese vector es posteriormente transformado con una base ortonormal orientada hacia la dirección de reflexión perfecta.

\medskip

La dirección del vector reflejado se calcula siguiendo el procedimiento explicado en la sección \ref{muestreo_brdf}. En caso de que el coseno entre la normal de la superficie y el vector resultado del muestreo sea inferior a 0, omitiremos este vector y asumiremos que su contribución de luz es nula. En caso contrario se traza el rayo, se evalúa la BRDF y se aplica la ponderación por muestreo de importancia.


\begin{figure}
\centering
\includegraphics[width=5in]{compare_specular2_burned.png}
\caption{Arriba exponente especular de 64, abajo exponente de 512. Se puede apreciar que cuanto mayor sea el exponente más reflectante será el material.}
\end{figure}

\clearpage

\section{Muestreo de luces}

Una mejora importante que hemos implementado ha sido el muestreo directo de fuentes de luz. Muestrear las luces directamente provoca que el renderizado converja más rápido al lanzar rayos hacia zonas de la escena que es más probable que aporten luz al cómputo final.

\begin{figure}[h]
\centering
\includegraphics[width=3in]{light_sampling.png}
\caption{Muestreo de luz directa}
\end{figure}

\clearpage

Para cada luz esférica calculamos la distancia del punto que estamos evaluando al centro de la esfera y en caso de encontrarnos dentro de la misma la omitimos y continuamos con la siguiente.

\medskip


A continuación procedemos a generar una dirección dentro del ángulo sólido subtendido por la esfera, tal como se ha explicado en la sección \ref{sample_solid}. Si dicha dirección forma un ángulo superior a noventa grados con respecto a la normal la omitimos.

\begin{figure}[h]
\centering
\includegraphics[scale=1.0]{sample_esfera.pdf}
\caption{  $\Delta $ al punto de intersección}
\end{figure}

La distancia a la que se encuentra el primer punto de intersección de la esfera con el rayo será $d_{intersect} = d \cos\theta - \Delta $. Este valor $\Delta = \sqrt{r^2 - (d\sin\theta)^2}$ lo calculamos en la instrucción \emph{float delta = sqrtf(radius2 - sin2theta * dist2);}. De este modo cuando creemos el rayo lo haremos con una distancia máxima que es la que nos interesa con tal de que no interseccione con objetos que estén dentro o detrás de la esfera. En las líneas 29 y 30 creamos y lanzamos el rayo. El valor \emph{shadow\_prd.contribution} será la emisión de luz de la fuente que estamos muestreando o $0$ si ha interseccionado con algún objeto.

\medskip

Finalmente en la última línea calculamos la contribución de luz de la muestra evaluando la BRDF.


\begin{figure}
\centering
\includegraphics[width=5in]{esferica.png}
\caption{Fuente de luz esférica}
\end{figure}

